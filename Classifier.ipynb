{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:46:53.147091Z",
     "start_time": "2025-09-25T14:46:53.131470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils import resample"
   ],
   "id": "ad7685c4c5b1af39",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:46:55.925596Z",
     "start_time": "2025-09-25T14:46:53.169281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# This dataset is accessible via the Hugging Face Hub. Download the datasets python package to read the URL from HuggingFace\n",
    "splits = {'train': 'sent_train.csv', 'validation': 'sent_valid.csv'}\n",
    "df_train = pd.read_csv(\"hf://datasets/zeroshot/twitter-financial-news-sentiment/\" + splits[\"train\"])\n",
    "df_val = pd.read_csv(\"hf://datasets/zeroshot/twitter-financial-news-sentiment/\" + splits[\"validation\"])\n",
    "\n",
    "# Rename for consistency\n",
    "df_train = df_train.rename(columns={'label':'labels'})\n",
    "df_val = df_val.rename(columns={'label':'labels'})\n"
   ],
   "id": "c6323a4ee91e4b9d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preparation",
   "id": "ca1143895681174b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:46:55.989669Z",
     "start_time": "2025-09-25T14:46:55.927592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "df_val['text'] = df_val['text'].apply(clean_text)\n",
    "new_tweets = [clean_text(t) for t in new_tweets]"
   ],
   "id": "e1743c1ca2413212",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define and Train Models",
   "id": "9f63bd4a463faf6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:48:07.577569Z",
     "start_time": "2025-09-25T14:46:55.989669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n",
    "X_train = vectorizer.fit_transform(df_train['text'])\n",
    "y_train = df_train['labels']\n",
    "\n",
    "X_val = vectorizer.transform(df_val['text'])\n",
    "y_val = df_val['labels']\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42),\n",
    "    \"Linear SVC\": LinearSVC(class_weight='balanced', max_iter=10000)\n",
    "}\n",
    "\n",
    "# Train models\n",
    "performance = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    performance[name] = acc\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_val, y_pred, target_names=['Bearish','Bullish','Neutral']))\n",
    "\n",
    "# Select Best Model\n",
    "\n",
    "best_model_name = max(performance, key=performance.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest model: {best_model_name} with Accuracy: {performance[best_model_name]:.4f}\")\n",
    "\n",
    "# Predictions on new tweets\n",
    "\n",
    "new_tweets = [\n",
    "    \"Stock markets are crashing hard\",\n",
    "    \"Company profits are skyrocketing\",\n",
    "    \"The situation remains unchanged\"\n",
    "]\n",
    "new_X = vectorizer.transform(new_tweets)\n",
    "preds = best_model.predict(new_X)\n",
    "\n",
    "reverse_label_map = {0:'Bearish', 1:'Bullish', 2:'Neutral'}\n",
    "predicted_sentiments = [reverse_label_map[p] for p in preds]\n",
    "\n",
    "print(\"\\nPredictions on new tweets:\")\n",
    "for tw, sentiment in zip(new_tweets, predicted_sentiments):\n",
    "    print(f\"{tw} → {sentiment}\")"
   ],
   "id": "fd887150b08e8b3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "Validation Accuracy: 0.7910\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Bearish       0.55      0.70      0.62       347\n",
      "     Bullish       0.67      0.72      0.69       475\n",
      "     Neutral       0.91      0.83      0.87      1566\n",
      "\n",
      "    accuracy                           0.79      2388\n",
      "   macro avg       0.71      0.75      0.73      2388\n",
      "weighted avg       0.81      0.79      0.80      2388\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "Validation Accuracy: 0.8086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Bearish       0.78      0.46      0.58       347\n",
      "     Bullish       0.81      0.54      0.65       475\n",
      "     Neutral       0.81      0.97      0.88      1566\n",
      "\n",
      "    accuracy                           0.81      2388\n",
      "   macro avg       0.80      0.66      0.70      2388\n",
      "weighted avg       0.81      0.81      0.79      2388\n",
      "\n",
      "\n",
      "Training Linear SVC...\n",
      "Validation Accuracy: 0.8149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Bearish       0.63      0.65      0.64       347\n",
      "     Bullish       0.71      0.71      0.71       475\n",
      "     Neutral       0.89      0.89      0.89      1566\n",
      "\n",
      "    accuracy                           0.81      2388\n",
      "   macro avg       0.74      0.75      0.74      2388\n",
      "weighted avg       0.82      0.81      0.82      2388\n",
      "\n",
      "\n",
      "Best model: Linear SVC with Accuracy: 0.8149\n",
      "\n",
      "Predictions on new tweets:\n",
      "Stock markets are crashing hard → Bearish\n",
      "Company profits are skyrocketing → Neutral\n",
      "The situation remains unchanged → Neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozzey\\miniconda3\\envs\\Assignment2\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we the LinearSVC performed the best on the validation set. However, traditional TF-IDF vectors may not capture the semantic meaning of the tweets as effectively as more advanced methods like word embeddings or transformer-based models.",
   "id": "6db991d0e051523b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Sentence Transformers for Embeddings \n",
    "Using this method, we can leverage pre-trained models to generate dense vector representations of the tweets, which can capture semantic meaning better than traditional TF-IDF vectors."
   ],
   "id": "b0bde7194829228b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:48:55.706172Z",
     "start_time": "2025-09-25T14:48:45.000469Z"
    }
   },
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer",
   "id": "73c5436fa80d1065",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:58:06.878109Z",
     "start_time": "2025-09-25T14:57:02.821729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "X_train = embedder.encode(df_train['text'], convert_to_tensor=False)\n",
    "X_val = embedder.encode(df_val['text'], convert_to_tensor=False)\n",
    "y_train = df_train['labels']\n",
    "y_val = df_val['labels']\n",
    "\n",
    "# Train classifier\n",
    "\n",
    "clf = LinearSVC(class_weight='balanced', max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "print(classification_report(y_val, y_pred, target_names=['Bearish','Bullish','Neutral']))\n",
    "\n",
    "# Predict new tweets\n",
    "\n",
    "new_tweets = [\n",
    "    \"Stock markets are crashing hard\",\n",
    "    \"Company profits are skyrocketing\",\n",
    "    \"The situation remains unchanged\"\n",
    "]\n",
    "\n",
    "new_tweets_clean = [clean_text(t) for t in new_tweets]\n",
    "new_X = embedder.encode(new_tweets_clean, convert_to_tensor=False)\n",
    "preds = clf.predict(new_X)\n",
    "\n",
    "reverse_label_map = {0:'Bearish', 1:'Bullish', 2:'Neutral'}\n",
    "predicted_sentiments = [reverse_label_map[p] for p in preds]\n",
    "\n",
    "print(\"\\nPredictions on new tweets:\")\n",
    "for tw, sentiment in zip(new_tweets, predicted_sentiments):\n",
    "    print(f\"{tw} → {sentiment}\")"
   ],
   "id": "3929aafd0f236c9d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozzey\\miniconda3\\envs\\Assignment2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\ozzey\\miniconda3\\envs\\Assignment2\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Bearish       0.59      0.66      0.62       347\n",
      "     Bullish       0.63      0.66      0.65       475\n",
      "     Neutral       0.86      0.83      0.85      1566\n",
      "\n",
      "    accuracy                           0.77      2388\n",
      "   macro avg       0.70      0.72      0.71      2388\n",
      "weighted avg       0.78      0.77      0.77      2388\n",
      "\n",
      "\n",
      "Predictions on new tweets:\n",
      "Stock markets are crashing hard → Bearish\n",
      "Company profits are skyrocketing → Bullish\n",
      "The situation remains unchanged → Neutral\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From the last output we can see that the model is able to classify the sentiment of new tweets based on the training it received from the financial news dataset. The use of sentence transformers allows for a more nuanced understanding of the text, potentially leading to better classification performance. The use of LinearSVC with class weights helps to address any class imbalance in the dataset and is better at capturing the decision boundaries in high-dimensional spaces.",
   "id": "5ee3a5ba85a2d1da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Function for Predicting Sentiment on New Tweets",
   "id": "a1f964ed39b86b9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:58:17.178379Z",
     "start_time": "2025-09-25T14:58:17.156417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_sentiment(tweets, classifier, embedder):\n",
    "    \"\"\"\n",
    "    Predicts sentiment for a list of tweets using a trained classifier and embedder.\n",
    "\n",
    "    Args:\n",
    "        tweets (list of str): Tweets to classify.\n",
    "        classifier: Trained scikit-learn classifier (e.g., LogisticRegression).\n",
    "        embedder: Trained SentenceTransformer model.\n",
    "\n",
    "    Prints:\n",
    "        Tweet → Predicted sentiment\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    # Clean text\n",
    "    def clean_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    tweets_clean = [clean_text(t) for t in tweets]\n",
    "\n",
    "    # Encode tweets\n",
    "    X = embedder.encode(tweets_clean, convert_to_tensor=False)\n",
    "\n",
    "    # Predict\n",
    "    preds = classifier.predict(X)\n",
    "\n",
    "    # Map back to labels\n",
    "    reverse_label_map = {0:'Bearish', 1:'Bullish', 2:'Neutral'}\n",
    "    predicted_sentiments = [reverse_label_map[p] for p in preds]\n",
    "\n",
    "    # Print results\n",
    "    for tw, sentiment in zip(tweets, predicted_sentiments):\n",
    "        print(f\"{tw} → {sentiment}\")\n"
   ],
   "id": "4a84d0f943e0cab9",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:58:17.378258Z",
     "start_time": "2025-09-25T14:58:17.342354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_tweets = [\n",
    "    \"markets are going down\",\n",
    "    \"Company income is going up\",\n",
    "    \"its a stable situation in the market\" \n",
    "]\n",
    "\n",
    "predict_sentiment(new_tweets, clf, embedder)\n"
   ],
   "id": "af2914d9e2659e32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markets are going down → Bearish\n",
      "Company income is going up → Bearish\n",
      "its a stable situation in the market → Neutral\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the model is able to classify the sentiment of new tweets based on the training it received from the financial news dataset. The use of sentence transformers allows for a more nuanced understanding of the text, potentially leading to better classification performance.",
   "id": "6c1c73f3845c0be9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save the model and embedder for future use",
   "id": "6fc11596230886ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T15:01:55.348608Z",
     "start_time": "2025-09-25T15:01:54.333895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open(\"clf.pkl\", \"wb\") as f: pickle.dump(clf, f)\n",
    "with open(\"embedder.pkl\", \"wb\") as f: pickle.dump(embedder, f)"
   ],
   "id": "11945d9f38969cac",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "79c34369d3100a88"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
